{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool, metrics, cv\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")  # 경고 출력 무시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/opt/ml/input/data/\"  # 경로는 상황에 맞춰서 수정해주세요!\n",
    "csv_file_path = os.path.join(data_dir, \"all_feature_data.csv\")  # 데이터는 대회홈페이지에서 받아주세요 :)\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "# 유저별 시퀀스를 고려하기 위해 아래와 같이 정렬\n",
    "df.sort_values(by=[\"userID\", \"Timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test 데이터 셋 분리 (option1, option2에서 하나만 실행)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1\n",
    "- train 데이터에서 train, valid set을 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test 데이터셋은 사용자 별로 묶어서 분리를 해주어야함\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def option1_train_test_split(df, ratio=0.8, split=True):\n",
    "\n",
    "    df = df[df.dataset == 1]\n",
    "\n",
    "    users = list(zip(df[\"userID\"].value_counts().index, df[\"userID\"].value_counts()))\n",
    "    random.shuffle(users)\n",
    "\n",
    "    max_train_data_len = ratio * len(df)\n",
    "    sum_of_train_data = 0\n",
    "    user_ids = []\n",
    "\n",
    "    for user_id, count in users:\n",
    "        sum_of_train_data += count\n",
    "        if max_train_data_len < sum_of_train_data:\n",
    "            break\n",
    "        user_ids.append(user_id)\n",
    "\n",
    "    train = df[df[\"userID\"].isin(user_ids)]\n",
    "    test = df[df[\"userID\"].isin(user_ids) == False]\n",
    "\n",
    "    # test데이터셋은 각 유저의 마지막 interaction만 추출\n",
    "    test = test[test[\"userID\"] != test[\"userID\"].shift(-1)]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2\n",
    "- train 데이터를 모두 훈련에 사용\n",
    "- valid를 test셋의 마지막 두번째 데이터로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option2_train_test_split(df):\n",
    "    # use train dataset only for train\n",
    "    train = df[df.dataset == 1]\n",
    "\n",
    "    # use test dataset only for valid\n",
    "    test = df[(df.dataset == 2)]  # & (df.answerCode != -1)]  # -1 인 answerCode 제외\n",
    "\n",
    "    # test데이터셋은 각 유저의 마지막 interaction만 추출\n",
    "    test = test[test[\"userID\"] != test[\"userID\"].shift(-1)]\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df, option=\"option1\"):\n",
    "\n",
    "    # 카테고리형 feature\n",
    "    categories = [\n",
    "        \"assessmentItemID\",\n",
    "        \"testId\",\n",
    "        \"KnowledgeTag\",\n",
    "        \"bigClassAccCate\",\n",
    "        \"bigClass\",\n",
    "        \"KTAccuracyCate\",\n",
    "        \"day\",\n",
    "        \"month\",\n",
    "        \"year\",\n",
    "        \"wday\",\n",
    "        \"weekNum\",\n",
    "        \"hour\",\n",
    "        \"elapsedTimeClass\",\n",
    "        \"tagCluster\",\n",
    "        \"testLV\",\n",
    "        \"userLVbyTest\",\n",
    "        \"userLVbyTestAVG\",\n",
    "        \"tagLV\",\n",
    "        \"userLVbyTag\",\n",
    "        \"userLVbyTagAVG\",\n",
    "        \"tagClass\",\n",
    "    ]  # TODO : category feature를 변환시켜줘야함\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    # df[\"elo\"] = df[\"elo\"].transform(lambda x: int(x * 100000000))\n",
    "\n",
    "    for category in categories:\n",
    "        if df[category].dtypes != \"int\":  # float, str type -> int로 전환\n",
    "            df[category] = le.fit_transform(df[category])\n",
    "        df[category] = df[category].astype(\"category\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feature_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = option1_train_test_split(df)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y 값 분리\n",
    "y_train = train[\"answerCode\"]\n",
    "train = train.drop([\"answerCode\"], axis=1)\n",
    "\n",
    "y_valid = valid[\"answerCode\"]\n",
    "valid = valid.drop([\"answerCode\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :사용할 Feature 설정\n",
    "FEATS = [\n",
    "    \"assessmentItemID\",\n",
    "    # \"testId\",\n",
    "    # \"KnowledgeTag\",\n",
    "    \"accuracy\",\n",
    "    # \"user_total_answer\",\n",
    "    \"testMean\",\n",
    "    # \"testSum\",\n",
    "    \"testStd\",\n",
    "    \"tagMean\",\n",
    "    # \"tagSum\",\n",
    "    \"tagStd\",\n",
    "    # \"assessSum\",\n",
    "    \"assessMean\",\n",
    "    \"assessStd\",\n",
    "    # -- 여기서부터 Custom Feature Engineering\n",
    "    # \"bigClass\",\n",
    "    \"bigClassAcc\",\n",
    "    \"bigClassElapsedTimeAvg\",\n",
    "    \"bigClassAccCate\",\n",
    "    # \"recAccuracy\",\n",
    "    \"cumAccuracy\",\n",
    "    # \"cumCorrect\",\n",
    "    # \"day\",\n",
    "    \"month\",\n",
    "    # \"year\",\n",
    "    # \"wday\",\n",
    "    # \"weekNum\",\n",
    "    # \"hour\",\n",
    "    \"elapsedTime\",\n",
    "    \"elapsedTimeClass\",\n",
    "    # \"KnowledgeTagAcc\",\n",
    "    # \"KTAccuracyCate\",\n",
    "    # \"seenCount\",\n",
    "    \"tagCluster\",\n",
    "    # \"tagCount\",\n",
    "    # \"testLV\",\n",
    "    # \"userLVbyTest\",\n",
    "    \"userLVbyTestAVG\",\n",
    "    # \"tagLV\",\n",
    "    # \"userLVbyTag\",\n",
    "    \"userLVbyTagAVG\",\n",
    "    # \"bigClassCount\",\n",
    "    \"recCount\",\n",
    "    \"elo\",\n",
    "    \"eloTest\",\n",
    "    \"eloTag\",\n",
    "    \"tagClass\",\n",
    "    \"GradeAcc\",\n",
    "    \"RepeatedTime\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = train[FEATS].columns[train[FEATS].dtypes == \"category\"].to_list()\n",
    "num_cols = train[FEATS].columns[train[FEATS].dtypes != \"category\"].to_list()\n",
    "\n",
    "print(f\"cat_cols: {cat_cols}\")\n",
    "print(f\"num_cols: {num_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 훈련 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(train[FEATS], y_train, cat_features=cat_cols)\n",
    "eval_pool = Pool(valid[FEATS], y_valid, cat_features=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"iterations\": 1500,\n",
    "    \"learning_rate\": 0.1,  # 0.1\n",
    "    \"eval_metric\": \"AUC\",\n",
    "    \"random_seed\": 42,\n",
    "    \"logging_level\": \"Silent\",\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"use_best_model\": True,\n",
    "    # \"task_type\": \"GPU\",\n",
    "    \"bagging_temperature\": 1,\n",
    "    \"cat_features\": cat_cols,\n",
    "}\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    **params,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train[FEATS],\n",
    "    y_train,\n",
    "    eval_set=[(valid[FEATS], y_valid)],\n",
    "    # cat_features=cat_cols,\n",
    "    plot=True,\n",
    "    # logging_level=\"Verbose\",  # you can uncomment this for text output\n",
    ")\n",
    "\n",
    "preds = model.predict_proba(valid[FEATS])[:, 1]\n",
    "acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n",
    "auc = roc_auc_score(y_valid, preds)\n",
    "\n",
    "print(f\"VALID AUC : {auc} ACC : {acc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Importance 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(\n",
    "    model,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_iter=1,\n",
    "    random_state=42,\n",
    "    cv=None,\n",
    "    refit=False,\n",
    ").fit(valid[FEATS], y_valid)\n",
    "eli5.show_weights(perm, top=len(FEATS), feature_names=FEATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(range(len(sorted_idx)), np.array(FEATS)[sorted_idx])\n",
    "plt.title(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1. Inferece by test [-2] (test dataset 뒤에서 두번째 값으로 성능 측정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test dataset only for valid\n",
    "test = df[(df.dataset == 2) & (df.answerCode != -1)]  # -1 인 answerCode 제외\n",
    "\n",
    "# test데이터셋은 각 유저의 마지막 interaction만 추출\n",
    "test = test[test[\"userID\"] != test[\"userID\"].shift(-1)]\n",
    "\n",
    "y_test = test[\"answerCode\"]\n",
    "test = test.drop([\"answerCode\"], axis=1)\n",
    "\n",
    "preds = model.predict_proba(test[FEATS])[:, 1]\n",
    "acc = accuracy_score(y_test, np.where(preds >= 0.5, 1, 0))\n",
    "auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "print(f\"VALID AUC : {auc} ACC : {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[df.dataset == 2]\n",
    "\n",
    "# LEAVE LAST INTERACTION ONLY\n",
    "test_df = test_df[test_df[\"userID\"] != test_df[\"userID\"].shift(-1)]\n",
    "\n",
    "# DROP ANSWERCODE\n",
    "test_df = test_df.drop([\"answerCode\"], axis=1)\n",
    "\n",
    "# MAKE PREDICTION\n",
    "total_preds = model.predict_proba(test_df[FEATS])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE OUTPUT\n",
    "output_dir = \"output/\"\n",
    "write_path = os.path.join(output_dir, \"CatBoost_submission_(8411).csv\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "with open(write_path, \"w\", encoding=\"utf8\") as w:\n",
    "    print(\"writing prediction : {}\".format(write_path))\n",
    "    w.write(\"id,prediction\\n\")\n",
    "    for id, p in enumerate(total_preds):\n",
    "        w.write(\"{},{}\\n\".format(id, p))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
